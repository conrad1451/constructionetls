# .github/workflows/etl_pipeline.yml

name: Construction ETL Pipeline

# CHQ: triggering workflow run

on:
  # Trigger the workflow on pushes to the 'main' branch (for testing/initial setup)
  push:
    branches:
      - main
    paths:
      - "etl_script.py"
      - "requirements.txt"
      - ".github/workflows/etl_pipeline.yml" # Rerun if workflow itself changes

  # Trigger the workflow on a schedule (e.g., daily at 02:00 UTC)
  # schedule:
  # Uses cron syntax. Learn more: https://crontab.guru/
  # new time of 2:52pm NYC time (1840 UTC) - 1440 + 4h offset
  # - cron: "52 18 * * *"
  # test time of 130am NYC time (530 UTC) - 0130 + 4h offest
  # - cron: "30 5 * * *"

  # test time of 640am NYC time (1040 UTC) - 0640 + 4h offest
  # - cron: "40 10 * * *"
  # test time of 310am NYC time (0710 UTC) - 0310 + 4h offest
  # - cron: "10 7 * * 4"
  # - cron: "0 0 * * 0"  # Runs every Sunday at midnight UTC # Google Search (can you make a yaml file run a job monthly?)
jobs:
  run_etl:
    runs-on: ubuntu-latest # Use a fresh Ubuntu runner for each job

    # Set environment variables from GitHub Secrets
    env:
      NEON_DB_HOST: ${{ secrets.NEON_DB_HOST }}
      NEON_DB_NAME: ${{ secrets.NEON_DB_NAME }}
      NEON_DB_USER: ${{ secrets.NEON_DB_USER }}
      NEON_DB_PASSWORD: ${{ secrets.NEON_DB_PASSWORD }}
      NEON_DB_PORT: ${{ secrets.NEON_DB_PORT }}
      XATA_DB_CONSTRUCTION: ${{ secrets.XATA_DB_CONSTRUCTION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4 # Action to check out your repository code

      - name: Set up Python
        uses: actions/setup-python@v5 # Action to set up Python environment
        with:
          python-version: "3.9" # Specify your Python version (e.g., 3.8, 3.9, 3.10)

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Clean up old log file
        run: rm -f etl_output.log # Deletes the log file if it exists

      - name: Run ETL Script
        id: run_script
        # run: python etl_past_day_script.py 2>&1 | tee etl_output.log
        run: python etl_script.py 2>&1 | tee etl_output.log

      - name: Display ETL log for debugging
        if: always()
        run: |
          echo "=== ETL Output Log ==="
          cat etl_output.log
          echo "=== End of Log ==="

      # CHQ: step added by Claude AI
      - name: Validate record count
        run: |
          # Check that we actually loaded records
          RECORD_COUNT=$(grep -oP "Successfully loaded \K\d+" etl_output.log | tail -1)
          if [ -z "$RECORD_COUNT" ] || [ "$RECORD_COUNT" -eq 0 ]; then
            echo "::error::No records were loaded"
            exit 1
          fi
          echo "✓ Loaded $RECORD_COUNT records"

      # CHQ: step added by Claude AI
      - name: Check for data validation errors
        run: |
          if grep -i "validation error\|invalid data\|assertion error" etl_output.log; then
            echo "::error::Data validation failed"
            exit 1
          fi
          echo "✓ Data validation passed"

      # CHQ: step added by Claude AI
      - name: Verify required columns present
        run: |
          if grep -i "KeyError\|missing column\|column not found" etl_output.log; then
            echo "::error::Required columns missing from data"
            exit 1
          fi
          echo "✓ All required columns present"

      # CHQ: step added by Gemini AI
      - name: Check ETL Results for Data Loading Success
        run: |
          if grep -q "No data to load as DataFrame is empty" etl_output.log; then
            echo "::error::ETL run considered a failure: No data was loaded into the database."
            exit 1 # Exit with a non-zero status to mark the job as failed
          else
            echo "ETL run completed with data loaded (or no 'no data' warning found)."
          fi

      - name: Check ETL Results for properly loaded records from API endpoint
        run: |
          if grep "Error for record at original index" etl_output.log; then
            echo "::error::ETL run considered a failure: Error for a record."
            exit 1 # Exit with a non-zero status to mark the job as failed
          else
            echo "ETL run completed with records loaded (or no 'no data' warning found)."
          fi

      - name: Check Database connection string to ensure it is correct
        run: |
          if grep "FATAL ERROR: DIGITAL_OCEAN_VM_DOCKER_HOSTED_SQL_ALT environment variable is NOT SET." etl_output.log; then
            echo "::error::ETL run considered a failure: Error for a record."
            exit 1 # Exit with a non-zero status to mark the job as failed
          else
            echo "ETL run completed with successful loading into database."
          fi
      - name: Check for new data
        # (Optional: for debugging)
        # You could add steps here to verify data in Neon, e.g., using psql
        # This step is just a placeholder to show where you might add checks.
        run: |
          echo "ETL script finished. Check Neon database for updated data."
